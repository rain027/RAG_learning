{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlLjoy46tZcXNVPf+rRd+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rain027/RAG_learning/blob/main/RAG_BASICS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install rank-bm25"
      ],
      "metadata": {
        "id": "myxCxNJ2PXo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install tiktoken   # for sentence tokenization and to include overlapping"
      ],
      "metadata": {
        "id": "X0g3y3alKV7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score bert-score"
      ],
      "metadata": {
        "id": "m9wxfGPqc7fW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "id": "EbPi-E1Uw4Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pulling a a rag article from wikipedia and feeding the cleaned version to the pipeline for better retrieval\n",
        "\n",
        "!pip install wikipedia-api\n",
        "import wikipediaapi\n",
        "\n",
        "# Add user_agent to follow Wikipedia's policy\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    user_agent=\"RAG-Experiment/1.0 (https://colab.research.google.com/)\",\n",
        "    language=\"en\"\n",
        ")\n",
        "\n",
        "page = wiki_wiki.page(\"Retrieval-augmented generation\")\n",
        "\n",
        "if page.exists():\n",
        "    text = page.text\n",
        "    print(\"Extracted characters:\", len(text))\n",
        "    print(text[:1000])  # preview first 1000 characters\n",
        "else:\n",
        "    print(\"Page not found\")\n",
        "\n",
        "# Clean text (optional, like before)\n",
        "import re\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "text = clean_text(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YwwS8B4Av8pW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u99uN73ZxiD-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder"
      ],
      "metadata": {
        "id": "56GxpTbrRwbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "H8ly-OVVI9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "-6WD6kl2Knhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n"
      ],
      "metadata": {
        "id": "X8taXWYidByK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "8AH43mulrKs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rAdF3uTqlLnx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Setup ChromaDB\n",
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.create_collection(name=\"docss\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sixga7fQPoec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.1 : token based chunking function\n",
        "def chunk_text_by_tokens(text, max_tokens=300, overlap=50):\n",
        "    tokens = word_tokenize(text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        chunk_tokens = tokens[start:start+max_tokens]\n",
        "        chunk_text = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        start += (max_tokens - overlap)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "wrzQwYF1K4pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text_for_evaluation(text, max_tokens=150, overlap=25):\n",
        "    \"\"\"\n",
        "    Create smaller chunks specifically for evaluation.\n",
        "    max_tokens: number of tokens per chunk\n",
        "    overlap: tokens overlapping between consecutive chunks\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        chunk_tokens = tokens[start:start+max_tokens]\n",
        "        chunk_text = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        start += (max_tokens - overlap)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "LV3nfZWhmmEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.3: chunking the loaded document stored in the variable text\n",
        "\n",
        "chunks = chunk_text_by_tokens(text, max_tokens=300, overlap=50)\n",
        "eval_chunks = chunk_text_for_evaluation(text, max_tokens=150, overlap=25)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "print(chunks[0])\n"
      ],
      "metadata": {
        "id": "lsPJE_bxPogo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Embed and Store chunks in DB\n",
        "for i, chunk in enumerate(chunks):\n",
        "    emb = model.encode(chunk).tolist()\n",
        "    collection.add(documents=[chunk], embeddings=[emb], ids=[str(i)])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dWUpqQj-PokP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare BM25\n",
        "tokenized_chunks = [c.lower().split() for c in chunks]\n",
        "bm25 = BM25Okapi(tokenized_chunks)\n"
      ],
      "metadata": {
        "id": "3YC94xflHwix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Cross Encoder\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
      ],
      "metadata": {
        "id": "OATrSa5zSDrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval function\n",
        "\n",
        "def retrieve_chunks(query, method=\"embedding\", top_k=5, use_hybrid=False, use_rerank=True):\n",
        "    \"\"\"\n",
        "    method: 'bm25' or 'embedding'\n",
        "    top_k: number of chunks to retrieve before optional reranking\n",
        "    use_hybrid: combine BM25 + embedding retrieval\n",
        "    use_rerank: apply cross-encoder re-ranking\n",
        "    \"\"\"\n",
        "    top_chunks = []\n",
        "\n",
        "    # --- Hybrid retrieval ---\n",
        "    if use_hybrid:\n",
        "        # Step 1: BM25 top-k\n",
        "        tokenized_query = query.lower().split()\n",
        "        bm25_scores = bm25.get_scores(tokenized_query)\n",
        "        top_bm25 = sorted(zip(chunks, bm25_scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        top_bm25_chunks = [c for c, s in top_bm25]\n",
        "\n",
        "        # Step 2: Embedding top-k\n",
        "        query_emb = model.encode(query).tolist()\n",
        "        embedding_results = collection.query(query_embeddings=[query_emb], n_results=top_k)\n",
        "        top_embedding_chunks = embedding_results[\"documents\"][0]\n",
        "\n",
        "        # Step 3: Merge (union) and remove duplicates\n",
        "        top_chunks = list(dict.fromkeys(top_bm25_chunks + top_embedding_chunks))\n",
        "\n",
        "    else:\n",
        "        # --- Single method retrieval ---\n",
        "        if method == \"embedding\":\n",
        "            query_emb = model.encode(query).tolist()\n",
        "            results = collection.query(query_embeddings=[query_emb], n_results=top_k)\n",
        "            top_chunks = results[\"documents\"][0]\n",
        "        elif method == \"bm25\":\n",
        "            tokenized_query = query.lower().split()\n",
        "            scores = bm25.get_scores(tokenized_query)\n",
        "            top_chunks = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "            top_chunks = [c for c, s in top_chunks]\n",
        "        else:\n",
        "            print(\"Invalid method! Choose 'bm25' or 'embedding'.\")\n",
        "            return None\n",
        "\n",
        "    # --- Optional cross-encoder re-ranking ---\n",
        "    if use_rerank:\n",
        "        rerank_scores = cross_encoder.predict([(query, c) for c in top_chunks])\n",
        "        top_chunks = [c for _, c in sorted(zip(rerank_scores, top_chunks), reverse=True)]\n",
        "\n",
        "\n",
        "    # --- Display top 3 chunks ---\n",
        "    print(\"\\n=== Top Chunks ===\")\n",
        "    for i, c in enumerate(top_chunks[:3]):\n",
        "        print(f\"{i+1}. {c}\\n\")\n",
        "\n",
        "    return top_chunks[:3]\n"
      ],
      "metadata": {
        "id": "GS1EXFzuV-n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(retrieved_chunks, reference_text, max_tokens=50):\n",
        "    \"\"\"\n",
        "    Evaluate retrieval using ROUGE and BERTScore, handling long chunks more gracefully.\n",
        "\n",
        "    retrieved_chunks: list of retrieved text chunks\n",
        "    reference_text: ground-truth text\n",
        "    max_tokens: max tokens to consider per chunk for evaluation\n",
        "    \"\"\"\n",
        "    # Step 1: Trim each retrieved chunk\n",
        "    trimmed_chunks = [\" \".join(c.split()[:max_tokens]) for c in retrieved_chunks]\n",
        "\n",
        "    # Step 2: Split reference into sentences\n",
        "    reference_sents = sent_tokenize(reference_text)\n",
        "\n",
        "    # Step 3: Build evaluation text by selecting chunks that have overlap with reference\n",
        "    eval_text = []\n",
        "    for chunk in trimmed_chunks:\n",
        "        for ref_sent in reference_sents:\n",
        "            if any(word.lower() in chunk.lower() for word in ref_sent.split()):\n",
        "                eval_text.append(chunk)\n",
        "                break\n",
        "    retrieved_text = \" \".join(eval_text)\n",
        "\n",
        "    # Step 4: Compute ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference_text, retrieved_text)\n",
        "    print(\"\\n--- ROUGE Scores ---\")\n",
        "    for key, value in rouge_scores.items():\n",
        "        print(f\"{key}: Precision={value.precision:.3f}, Recall={value.recall:.3f}, F1={value.fmeasure:.3f}\")\n",
        "\n",
        "    # Step 5: Compute BERTScore\n",
        "    P, R, F1 = bert_score([retrieved_text], [reference_text], lang='en', model_type='roberta-large', rescale_with_baseline=True)\n",
        "    print(f\"\\nBERTScore F1: {F1[0].item():.3f}\")"
      ],
      "metadata": {
        "id": "ES27-BN-dKWo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Query\n",
        "query = \"What is retrieval augmented generation?\"\n",
        "\n",
        "reference_text = \"\"\"Retrieval-augmented generation ( RAG ) is a technique that enables large language models ( LLMs ) to retrieve and incorporate new information . With RAG , LLMs do not respond to user queries until they refer to a specified set of documents . These documents supplement information from the LLM 's pre-existing training data . This allows LLMs to use domain-specific and/or updated information that is not available in the training data . For example , this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources . RAG improves large language models ( LLMs ) by incorporating information retrieval before generating responses . Unlike traditional LLMs that rely on static training data , RAG pulls relevant text from databases , uploaded documents , or web sources . According to Ars Technica , `` RAG is a way of improving LLM performance , in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts . '' This method helps reduce AI hallucinations , which have caused chatbots to describe policies that do not exist , or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments . RAG also reduces the need to retrain LLMs with new data , saving on computational and financial costs . Beyond efficiency gains , RAG also allows LLMs to include sources in their responses , so users can verify the cited sources . This provides greater transparency , as users can cross-check retrieved content to ensure accuracy and relevance . The term RAG was first introduced in a 2020 research paper from Meta .\"\"\"\n",
        "\n",
        "# Ask user if they want to apply hybrid retrieval\n",
        "use_hybrid = input(\"Do you want to use hybrid retrieval (BM25 + embeddings)? (y/n): \").strip().lower() == \"y\"\n",
        "\n",
        "# Ask user if they want to apply cross-encoder reranking\n",
        "use_rerank = input(\"Do you want to apply cross-encoder reranking? (y/n): \").strip().lower() == \"y\"\n",
        "\n",
        "# Ask user which single method to use if not hybrid\n",
        "method = \"embedding\"\n",
        "if not use_hybrid:\n",
        "    method_input = input(\"Choose retrieval method (bm25 / embedding): \").strip().lower()\n",
        "    if method_input in [\"bm25\", \"embedding\"]:\n",
        "        method = method_input\n",
        "\n",
        "\n",
        "retrieved_chunks = retrieve_chunks(query, method=\"embedding\", top_k=5, use_hybrid=True, use_rerank=True)\n",
        "evaluate_retrieval(retrieved_chunks, reference_text, max_tokens=50)"
      ],
      "metadata": {
        "id": "VDeT5KOlP0Ag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ulrFmZPwH4KW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}