{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlLjoy46tZcXNVPf+rRd+i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rain027/RAG_learning/blob/main/RAG_BASICS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb\n",
        "!pip install rank-bm25"
      ],
      "metadata": {
        "id": "myxCxNJ2PXo3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32560698-1793-4d47-ef25-5c58a3a4e537"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.12/dist-packages (1.0.20)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.12/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.36.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (1.74.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.17.3)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb) (4.25.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.4)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.36.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.36.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.57b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.57b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb) (0.34.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.9)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: rank-bm25 in /usr/local/lib/python3.12/dist-packages (0.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rank-bm25) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install tiktoken   # for sentence tokenization and to include overlapping"
      ],
      "metadata": {
        "id": "X0g3y3alKV7L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a994494e-dd88-43de-d176-163972bbb5d5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.11.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score bert-score"
      ],
      "metadata": {
        "id": "m9wxfGPqc7fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcf929a8-a513-4ead-e378-640ad96c80c1"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.8.0+cu126)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.2.2)\n",
            "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.56.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert-score) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert-score) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.34.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert-score) (0.6.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (1.5.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.8.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert-score) (1.1.9)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert-score) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbPi-E1Uw4Bd",
        "outputId": "06fb3a51-2a3a-45b2-d634-ed1468c38c29"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pulling a a rag article from wikipedia and feeding the cleaned version to the pipeline for better retrieval\n",
        "\n",
        "!pip install wikipedia-api\n",
        "import wikipediaapi\n",
        "\n",
        "# Add user_agent to follow Wikipedia's policy\n",
        "wiki_wiki = wikipediaapi.Wikipedia(\n",
        "    user_agent=\"RAG-Experiment/1.0 (https://colab.research.google.com/)\",\n",
        "    language=\"en\"\n",
        ")\n",
        "\n",
        "page = wiki_wiki.page(\"Retrieval-augmented generation\")\n",
        "\n",
        "if page.exists():\n",
        "    text = page.text\n",
        "    print(\"Extracted characters:\", len(text))\n",
        "    print(text[:1000])  # preview first 1000 characters\n",
        "else:\n",
        "    print(\"Page not found\")\n",
        "\n",
        "# Clean text (optional, like before)\n",
        "import re\n",
        "def clean_text(text):\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "text = clean_text(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwwS8B4Av8pW",
        "outputId": "248ee550-d58a-4d38-aaa1-22b13118de31"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia-api in /usr/local/lib/python3.12/dist-packages (0.8.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from wikipedia-api) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->wikipedia-api) (2025.8.3)\n",
            "Extracted characters: 11860\n",
            "Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\n",
            "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "u99uN73ZxiD-"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import CrossEncoder"
      ],
      "metadata": {
        "id": "56GxpTbrRwbm"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi"
      ],
      "metadata": {
        "id": "H8ly-OVVI9xB"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "-6WD6kl2Knhu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "420c3776-3ed8-4fbd-e969-442b88092087"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score as bert_score\n"
      ],
      "metadata": {
        "id": "X8taXWYidByK"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "8AH43mulrKs1"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Load model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rAdF3uTqlLnx"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Setup ChromaDB\n",
        "chroma_client = chromadb.Client()\n",
        "collection = chroma_client.create_collection(name=\"docss\")\n",
        "\n"
      ],
      "metadata": {
        "id": "sixga7fQPoec"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.1 : token based chunking function\n",
        "def chunk_text_by_tokens(text, max_tokens=300, overlap=50):\n",
        "    tokens = word_tokenize(text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        chunk_tokens = tokens[start:start+max_tokens]\n",
        "        chunk_text = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        start += (max_tokens - overlap)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "wrzQwYF1K4pd"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text_for_evaluation(text, max_tokens=150, overlap=25):\n",
        "    \"\"\"\n",
        "    Create smaller chunks specifically for evaluation.\n",
        "    max_tokens: number of tokens per chunk\n",
        "    overlap: tokens overlapping between consecutive chunks\n",
        "    \"\"\"\n",
        "    tokens = word_tokenize(text)\n",
        "    chunks = []\n",
        "    start = 0\n",
        "    while start < len(tokens):\n",
        "        chunk_tokens = tokens[start:start+max_tokens]\n",
        "        chunk_text = \" \".join(chunk_tokens)\n",
        "        chunks.append(chunk_text)\n",
        "        start += (max_tokens - overlap)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "LV3nfZWhmmEi"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3.3: chunking the loaded document stored in the variable text\n",
        "\n",
        "chunks = chunk_text_by_tokens(text, max_tokens=300, overlap=50)\n",
        "eval_chunks = chunk_text_for_evaluation(text, max_tokens=150, overlap=25)\n",
        "print(f\"Total chunks created: {len(chunks)}\")\n",
        "print(chunks[0])\n"
      ],
      "metadata": {
        "id": "lsPJE_bxPogo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7c7c63c-7a9d-4b2f-d4c0-f440a99c1317"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total chunks created: 9\n",
            "Retrieval-augmented generation ( RAG ) is a technique that enables large language models ( LLMs ) to retrieve and incorporate new information . With RAG , LLMs do not respond to user queries until they refer to a specified set of documents . These documents supplement information from the LLM 's pre-existing training data . This allows LLMs to use domain-specific and/or updated information that is not available in the training data . For example , this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources . RAG improves large language models ( LLMs ) by incorporating information retrieval before generating responses . Unlike traditional LLMs that rely on static training data , RAG pulls relevant text from databases , uploaded documents , or web sources . According to Ars Technica , `` RAG is a way of improving LLM performance , in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts . '' This method helps reduce AI hallucinations , which have caused chatbots to describe policies that do n't exist , or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments . RAG also reduces the need to retrain LLMs with new data , saving on computational and financial costs . Beyond efficiency gains , RAG also allows LLMs to include sources in their responses , so users can verify the cited sources . This provides greater transparency , as users can cross-check retrieved content to ensure accuracy and relevance . The term RAG was first introduced in a 2020 research paper from Meta . RAG and LLM Limitations LLMs can provide incorrect information . For example , when Google first demonstrated its LLM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Embed and Store chunks in DB\n",
        "for i, chunk in enumerate(chunks):\n",
        "    emb = model.encode(chunk).tolist()\n",
        "    collection.add(documents=[chunk], embeddings=[emb], ids=[str(i)])\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dWUpqQj-PokP"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare BM25\n",
        "tokenized_chunks = [c.lower().split() for c in chunks]\n",
        "bm25 = BM25Okapi(tokenized_chunks)\n"
      ],
      "metadata": {
        "id": "3YC94xflHwix"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading Cross Encoder\n",
        "cross_encoder = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
      ],
      "metadata": {
        "id": "OATrSa5zSDrq"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieval function\n",
        "\n",
        "def retrieve_chunks(query, method=\"embedding\", top_k=5, use_hybrid=False, use_rerank=True):\n",
        "    \"\"\"\n",
        "    method: 'bm25' or 'embedding'\n",
        "    top_k: number of chunks to retrieve before optional reranking\n",
        "    use_hybrid: combine BM25 + embedding retrieval\n",
        "    use_rerank: apply cross-encoder re-ranking\n",
        "    \"\"\"\n",
        "    top_chunks = []\n",
        "\n",
        "    # --- Hybrid retrieval ---\n",
        "    if use_hybrid:\n",
        "        # Step 1: BM25 top-k\n",
        "        tokenized_query = query.lower().split()\n",
        "        bm25_scores = bm25.get_scores(tokenized_query)\n",
        "        top_bm25 = sorted(zip(chunks, bm25_scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "        top_bm25_chunks = [c for c, s in top_bm25]\n",
        "\n",
        "        # Step 2: Embedding top-k\n",
        "        query_emb = model.encode(query).tolist()\n",
        "        embedding_results = collection.query(query_embeddings=[query_emb], n_results=top_k)\n",
        "        top_embedding_chunks = embedding_results[\"documents\"][0]\n",
        "\n",
        "        # Step 3: Merge (union) and remove duplicates\n",
        "        top_chunks = list(dict.fromkeys(top_bm25_chunks + top_embedding_chunks))\n",
        "\n",
        "    else:\n",
        "        # --- Single method retrieval ---\n",
        "        if method == \"embedding\":\n",
        "            query_emb = model.encode(query).tolist()\n",
        "            results = collection.query(query_embeddings=[query_emb], n_results=top_k)\n",
        "            top_chunks = results[\"documents\"][0]\n",
        "        elif method == \"bm25\":\n",
        "            tokenized_query = query.lower().split()\n",
        "            scores = bm25.get_scores(tokenized_query)\n",
        "            top_chunks = sorted(zip(chunks, scores), key=lambda x: x[1], reverse=True)[:top_k]\n",
        "            top_chunks = [c for c, s in top_chunks]\n",
        "        else:\n",
        "            print(\"Invalid method! Choose 'bm25' or 'embedding'.\")\n",
        "            return None\n",
        "\n",
        "    # --- Optional cross-encoder re-ranking ---\n",
        "    if use_rerank:\n",
        "        rerank_scores = cross_encoder.predict([(query, c) for c in top_chunks])\n",
        "        top_chunks = [c for _, c in sorted(zip(rerank_scores, top_chunks), reverse=True)]\n",
        "\n",
        "\n",
        "    # --- Display top 3 chunks ---\n",
        "    print(\"\\n=== Top Chunks ===\")\n",
        "    for i, c in enumerate(top_chunks[:3]):\n",
        "        print(f\"{i+1}. {c}\\n\")\n",
        "\n",
        "    return top_chunks[:3]\n"
      ],
      "metadata": {
        "id": "GS1EXFzuV-n5"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_retrieval(retrieved_chunks, reference_text, max_tokens=50):\n",
        "    \"\"\"\n",
        "    Evaluate retrieval using ROUGE and BERTScore, handling long chunks more gracefully.\n",
        "\n",
        "    retrieved_chunks: list of retrieved text chunks\n",
        "    reference_text: ground-truth text\n",
        "    max_tokens: max tokens to consider per chunk for evaluation\n",
        "    \"\"\"\n",
        "    # Step 1: Trim each retrieved chunk\n",
        "    trimmed_chunks = [\" \".join(c.split()[:max_tokens]) for c in retrieved_chunks]\n",
        "\n",
        "    # Step 2: Split reference into sentences\n",
        "    reference_sents = sent_tokenize(reference_text)\n",
        "\n",
        "    # Step 3: Build evaluation text by selecting chunks that have overlap with reference\n",
        "    eval_text = []\n",
        "    for chunk in trimmed_chunks:\n",
        "        for ref_sent in reference_sents:\n",
        "            if any(word.lower() in chunk.lower() for word in ref_sent.split()):\n",
        "                eval_text.append(chunk)\n",
        "                break\n",
        "    retrieved_text = \" \".join(eval_text)\n",
        "\n",
        "    # Step 4: Compute ROUGE\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = scorer.score(reference_text, retrieved_text)\n",
        "    print(\"\\n--- ROUGE Scores ---\")\n",
        "    for key, value in rouge_scores.items():\n",
        "        print(f\"{key}: Precision={value.precision:.3f}, Recall={value.recall:.3f}, F1={value.fmeasure:.3f}\")\n",
        "\n",
        "    # Step 5: Compute BERTScore\n",
        "    P, R, F1 = bert_score([retrieved_text], [reference_text], lang='en', model_type='roberta-large', rescale_with_baseline=True)\n",
        "    print(f\"\\nBERTScore F1: {F1[0].item():.3f}\")"
      ],
      "metadata": {
        "id": "ES27-BN-dKWo"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Query\n",
        "query = \"What is retrieval augmented generation?\"\n",
        "\n",
        "reference_text = \"\"\"Retrieval-augmented generation ( RAG ) is a technique that enables large language models ( LLMs ) to retrieve and incorporate new information . With RAG , LLMs do not respond to user queries until they refer to a specified set of documents . These documents supplement information from the LLM 's pre-existing training data . This allows LLMs to use domain-specific and/or updated information that is not available in the training data . For example , this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources . RAG improves large language models ( LLMs ) by incorporating information retrieval before generating responses . Unlike traditional LLMs that rely on static training data , RAG pulls relevant text from databases , uploaded documents , or web sources . According to Ars Technica , `` RAG is a way of improving LLM performance , in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts . '' This method helps reduce AI hallucinations , which have caused chatbots to describe policies that do not exist , or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments . RAG also reduces the need to retrain LLMs with new data , saving on computational and financial costs . Beyond efficiency gains , RAG also allows LLMs to include sources in their responses , so users can verify the cited sources . This provides greater transparency , as users can cross-check retrieved content to ensure accuracy and relevance . The term RAG was first introduced in a 2020 research paper from Meta .\"\"\"\n",
        "\n",
        "# Ask user if they want to apply hybrid retrieval\n",
        "use_hybrid = input(\"Do you want to use hybrid retrieval (BM25 + embeddings)? (y/n): \").strip().lower() == \"y\"\n",
        "\n",
        "# Ask user if they want to apply cross-encoder reranking\n",
        "use_rerank = input(\"Do you want to apply cross-encoder reranking? (y/n): \").strip().lower() == \"y\"\n",
        "\n",
        "# Ask user which single method to use if not hybrid\n",
        "method = \"embedding\"\n",
        "if not use_hybrid:\n",
        "    method_input = input(\"Choose retrieval method (bm25 / embedding): \").strip().lower()\n",
        "    if method_input in [\"bm25\", \"embedding\"]:\n",
        "        method = method_input\n",
        "\n",
        "\n",
        "retrieved_chunks = retrieve_chunks(query, method=\"embedding\", top_k=5, use_hybrid=True, use_rerank=True)\n",
        "evaluate_retrieval(retrieved_chunks, reference_text, max_tokens=50)"
      ],
      "metadata": {
        "id": "VDeT5KOlP0Ag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60fbcde3-2018-4e4f-edd1-500aa3443b44"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Do you want to use hybrid retrieval (BM25 + embeddings)? (y/n): y\n",
            "Do you want to apply cross-encoder reranking? (y/n): y\n",
            "\n",
            "=== Top Chunks ===\n",
            "1. Retrieval-augmented generation ( RAG ) is a technique that enables large language models ( LLMs ) to retrieve and incorporate new information . With RAG , LLMs do not respond to user queries until they refer to a specified set of documents . These documents supplement information from the LLM 's pre-existing training data . This allows LLMs to use domain-specific and/or updated information that is not available in the training data . For example , this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources . RAG improves large language models ( LLMs ) by incorporating information retrieval before generating responses . Unlike traditional LLMs that rely on static training data , RAG pulls relevant text from databases , uploaded documents , or web sources . According to Ars Technica , `` RAG is a way of improving LLM performance , in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts . '' This method helps reduce AI hallucinations , which have caused chatbots to describe policies that do n't exist , or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments . RAG also reduces the need to retrain LLMs with new data , saving on computational and financial costs . Beyond efficiency gains , RAG also allows LLMs to include sources in their responses , so users can verify the cited sources . This provides greater transparency , as users can cross-check retrieved content to ensure accuracy and relevance . The term RAG was first introduced in a 2020 research paper from Meta . RAG and LLM Limitations LLMs can provide incorrect information . For example , when Google first demonstrated its LLM\n",
            "\n",
            "2. LLM with key information early in the prompt , encouraging it to prioritize the supplied data over pre-existing training knowledge . Process Retrieval-augmented generation ( RAG ) enhances large language models ( LLMs ) by incorporating an information-retrieval mechanism that allows models to access and utilize additional data beyond their original training set . AWS states , `` RAG allows LLMs to retrieve relevant information from external data sources to generate more accurate and contextually relevant responses '' ( `` indexing '' ) . This approach reduces reliance on static datasets , which can quickly become outdated . When a user submits a query , RAG uses a document retriever to search for relevant content from available sources before incorporating the retrieved information into the model 's response ( `` retrieval '' ) . Ars Technica notes that `` when new information becomes available , rather than having to retrain the model , all that ’ s needed is to augment the model ’ s external knowledge base with the updated information '' ( `` augmentation '' ) . By dynamically integrating relevant data , RAG enables LLMs to generate more informed and contextually grounded responses ( `` generation '' ) . IBM states that `` in the generative phase , the LLM draws from the augmented prompt and its internal representation of its training data to synthesize an engaging answer tailored to the user in that instant . RAG key stages Indexing Typically , the data to be referenced is converted into LLM embeddings , numerical representations in the form of a large vector space . RAG can be used on unstructured ( usually text ) , semi-structured , or structured data ( for example knowledge graphs ) . These embeddings are then stored in a vector database to\n",
            "\n",
            "3. is converted into LLM embeddings , numerical representations in the form of a large vector space . RAG can be used on unstructured ( usually text ) , semi-structured , or structured data ( for example knowledge graphs ) . These embeddings are then stored in a vector database to allow for document retrieval . Retrieval Given a user query , a document retriever is first called to select the most relevant documents that will be used to augment the query . This comparison can be done using a variety of methods , which depend in part on the type of indexing used . Augmentation The model feeds this relevant retrieved information into the LLM via prompt engineering of the user 's original query . Newer implementations ( as of 2023 ) can also incorporate specific augmentation modules with abilities such as expanding queries into multiple domains and using memory and self-improvement to learn from previous retrievals . Generation Finally , the LLM can generate output based on both the query and the retrieved documents . Some models incorporate extra steps to improve output , such as the re-ranking of retrieved information , context selection , and fine-tuning . Improvements Improvements to the basic process above can be applied at different stages in the RAG flow . Encoder These methods focus on the encoding of text as either dense or sparse vectors . Sparse vectors , which encode the identity of a word , are typically dictionary-length and contain mostly zeros . Dense vectors , which encode meaning , are more compact and contain fewer zeros . Various enhancements can improve the way similarities are calculated in the vector stores ( databases ) . Performance improves by optimizing how vector similarities are calculated . Dot products enhance similarity scoring ,\n",
            "\n",
            "\n",
            "--- ROUGE Scores ---\n",
            "rouge1: Precision=0.714, Recall=0.374, F1=0.491\n",
            "rouge2: Precision=0.424, Recall=0.221, F1=0.291\n",
            "rougeL: Precision=0.526, Recall=0.276, F1=0.362\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "BERTScore F1: 0.270\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ulrFmZPwH4KW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}